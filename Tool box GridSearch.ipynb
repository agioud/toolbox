{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlJ+3aqIqDt5sE6VJopKQL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import warnings    #enlever les warning\n","\n","warnings.filterwarnings(\"ignore\")\n","pd.set_option('display.max_colwidth', None) #sup tronquage  df"],"metadata":{"id":"rgIHTlClQ9Vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piuVpR4uVkCF"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","models = [\n","    LogisticRegression(),\n","    DecisionTreeClassifier()\n","]\n","param_grids = [\n","    # Paramètres pour la régression logistique\n","    {'C': [0.01, 0.1, 1, 10, 100],  # Hyperparamètre de régularisation\n","     'solver': ['liblinear', 'saga'],  # Solvers possibles pour la régression logistique\n","     'max_iter': [100, 200]},  # Nombre maximum d'itérations pour l'optimisation\n","    # Paramètres pour l'arbre de décision\n","    {'max_depth': [None, 5, 10, 20],  # Profondeur maximale de l'arbre\n","     'min_samples_split': [2, 5, 10],  # Nombre minimum d'échantillons nécessaires pour diviser un nœud\n","     'min_samples_leaf': [1, 2, 5]}  # Nombre minimum d'échantillons dans chaque feuille\n","]\n","result = []\n","for model, param_grid in zip(models, param_grids):\n","    gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","    gs.fit(X_train_vecto, y_train)\n","    # Pour chaque modele on affiche le meilleur score obtenu ainsi que les paramètres qui donnent ce score\n","    print(f\"Meilleur score pour {model.__class__.__name__}: {gs.best_score_:.3f}\")\n","    print(f\"Meilleurs paramètres pour {model.__class__.__name__}: {gs.best_params_}\")\n","    result.append({\n","        'Model': model.__class__.__name__,\n","        'Best Score': gs.best_score_,\n","        'Best Parameters': gs.best_params_\n","    })\n","results_df = pd.DataFrame(result).sort_values(by='Best Score', ascending=False)"]},{"cell_type":"code","source":["# Initialiser les modèles\n","models = [\n","  KNeighborsClassifier(),\n","  SVC(),\n","  SGDClassifier(),\n","  MLPClassifier(max_iter=1000)]\n","\n","# Définir les grilles de paramètres pour chaque modèle\n","param_grids = [\n","  {'n_neighbors': [3, 5, 7, 9]},\n","  {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n","  {'loss': ['hinge', 'log'], 'alpha': [0.0001, 0.001, 0.01]},\n","  {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu']}\n","]\n","\n","# Liste pour stocker les résultats\n","result = []\n","\n","# Effectuer la recherche de grille pour chaque modèle\n","for model, param_grid in zip(models, param_grids):\n","    gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","    gs.fit(X_train, y_train)  # Ajuster le modèle sur les données d'entraînement\n","\n","    # Afficher le meilleur score et les meilleurs paramètres\n","    print(f\"Meilleur score pour {model.__class__.__name__}: {gs.best_score_:.3f}\")\n","    print(f\"Meilleurs paramètres pour {model.__class__.__name__}: {gs.best_params_}\")\n","\n","    # Ajouter les résultats à la liste\n","    result.append({\n","        'Model': model.__class__.__name__,\n","        'Best Score': gs.best_score_,\n","        'Best Parameters': gs.best_params_\n","    })\n","\n","# Convertir les résultats en DataFrame et trier par score\n","results_df = pd.DataFrame(result).sort_values(by='Best Score', ascending=False)\n","\n","# Afficher le DataFrame des résultats\n","print(\"\\nRésultats de la recherche de grille :\")\n","display(results_df)"],"metadata":{"id":"xEz9sv22RqSQ"},"execution_count":null,"outputs":[]}]}